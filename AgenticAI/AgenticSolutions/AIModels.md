## [Choose and deploy models from the model catalog in Azure AI Foundry portal](https://learn.microsoft.com/en-us/training/modules/explore-models-azure-ai-studio/)

### Can AI solve my use case?
- You can explore the available language models through three different catalogs:
    - [Azure AI Foundry](https://ai.azure.com/explore/models): Comprehensive catalog with robust tools for deployment.
    - [Hugging Face](https://huggingface.co/models): Vast catalog of open-source models across various domains.
    - [GitHub](https://github.com/marketplace/models-github): Access to diverse models via GitHub Marketplace and GitHub Copilot.
- Choose between large and small language models
- Focus on a modality, task, or tool
    - Chat completion models (GPT-4)
    - Reasoning models (DeepSeek-R1, o1)
    - Multi-modal moduels (GPT-4o)
    - Image generation models (Dall-E 3)
    - Embedding models (Ada, Cohere)
- Specialize with regional and domain-specific models
- Balance flexibility and performance with open versus proprietary models

### How do I select the best model for my use case?
- Filter models for precision
- Filter models for performance

#### Benchmarks
Benchmark|Description
--|--
**Accuracy** | Compares model-generated text with correct answer according to the dataset. Result is one if generated text matches the answer exactly, and zero otherwise.
**Coherence** | Measures whether the model output flows smoothly, reads naturally, and resembles human-like language.
**Fluency** | Assesses how well the generated text adheres to grammatical rules, syntactic structures, and appropriate usage of vocabulary, resulting in linguistically correct and natural-sounding responses.
**Groundedness** | Measures alignment between the model's generated answers and the input data.
**GPT Similarity** | Quantifies the semantic similarity between a ground truth sentence (or document) and the prediction sentence generated by an AI model.
**Quality index** | A comparative aggregate score between 0 and 1, with better-performing models scoring a higher value.
**Cost** | The cost of using the model based on a price-per-token. Cost is a useful metric with which to compare quality, enabling you to determine an appropriate tradeoff for your needs.

### Can I scale for real-world workloads?
- `Model deployment`: Where will you deploy the model for the best balance of performance and cost?
- `Model monitoring and optimization`: How will you monitor, evaluate, and optimize model performance?
- `Prompt management`: How will you orchestrate and optimize prompts to maximize the accuracy and relevance of generated responses?
- `Model lifecycle`: How will you manage model, data, and code updates as part of an ongoing Generative AI Operations (GenAIOps) lifecycle?

### Deploying a model 
- `https://ai-aihubdevdemo.openai.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2023-03-15-preview`
- URI includes hub name, model name, task
- You can deploy:
    - [Azure OpenAI models](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions) like GPT-3.5 and GPT-4 with Azure OpenAI service and Azure AI model inference.
    - Third-party models like DeepSeek-R1 as [Models as a Service](https://learn.microsoft.com/en-us/azure/ai-foundry/model-inference/concepts/models) as part of Azure AI model inference or with [serverless APIs](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/model-catalog-overview#model-deployment-managed-compute-and-serverless-apis).
    - Open and custom models like models from Hugging Face with your own [user-managed compute](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/model-catalog-overview#managed-compute).

### [Optimize model performance](https://learn.microsoft.com/en-us/training/modules/explore-models-azure-ai-studio/4-improve-model)

#### Apply prompt patterns to optimize your model's output
- Instruct the model to act as a persona.
- Guide the model to suggest better questions.
- Provide a template to generate output in a specific format.
- Understand how a model reasons by asking it to reflect.
- Add context to improve the accuracy of the model's output.
- [A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT](https://arxiv.org/abs/2302.11382)

#### Apply model optimization strategies
- **Retrieval Augmented Generation (RAG)**
    - A technique that involves using a data source to provide grounding context to prompts.
    - RAG can be a useful approach when you need the model to answer questions based on a specific knowledge domain or when you need the model to consider information related to events that occurred after the training data on which the model is based.
- **Fine-tuning**
    - A technique that involves extending the training of a foundation model by providing example prompts and responses that reflect the desired output format and style.

### [Exercise - Explore, deploy, and chat with language models](https://learn.microsoft.com/en-us/training/modules/explore-models-azure-ai-studio/5-exercise)

### Links
- [Model catalog and collections in Azure AI Foundry portal](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/model-catalog-overview)
- [Overview: Deploy AI models in Azure AI Foundry portal](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/deployments-overview)
- [Prompt engineering techniques](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering?tabs=chat)
